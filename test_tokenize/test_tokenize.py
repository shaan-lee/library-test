from collections import Counter
import re

# Sample unknown language text (replace with your text)
text = """
[ ë„ˆë¥¼ ë§Œë‚˜ë‹¤ ] 148(+153)í™”ê°€ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤ğŸ¥°
í•­ìƒ ìŠì§€ ì•Šê³  ì±™ê²¨ë´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤ à¥± á—œ à¥± áƒ¦

ë¬´ë£ŒíšŒì°¨ì— ë‹¨ì²´ì‚¬ì§„ í’€ë ¤ì„œ ì˜¬ë¦¬ê³ ê°‘ë‹ˆë‹¤ ã…ã…
ì›ë³¸ ìŠ¤í† ë¦¬ì—ë„ ì˜¬ë ¤ë†“ì„ê²Œìš”
ëŠ¦ì–´ì„œ ì£„ì†¡í•©ë‹ˆë‹¤!

#ë„ˆë¥¼ë§Œë‚˜ë‹¤ #å¶ç„¶å‡ºä¼šã£ãŸå› #ì¹´ì¹´ì˜¤ì›¹íˆ° #ì¹´ì¹´ì˜¤í˜ì´ì§€ #ãƒ”ãƒƒã‚³ãƒ #piccoma  #ìˆœì •ë§Œí™” #ì›¹íˆ° 
#kakaowebtoon #kakaopage #webtoon #illust #illustration 
#drawing #manhwa #romance #artwork

ëª¨ë“  ê²Œ ì²˜ìŒì´ë¼ ë¶€ë„ëŸ¬ìš´ ì—°í•˜ê³µê³¼
ê·¸ëŸ° ë™ìƒì„ ë†€ë ¤ë¨¹ëŠ” í˜•ì•„ğŸ˜†

ğŸ”ë¯¸ìŠ¤í„°ë¸”ë£¨ ê²€ìƒ‰ì°½ì—
ğŸ’“ë“œë¡­ì•„ì›ƒğŸ’“ì„ ê²€ìƒ‰í•´ ë³´ì„¸ìš”!

#bl #blì¶”ì²œ #webtoon #blwebtoon
#BL #ì—°ì˜ˆê³„ #ì‚¬ë‘ê¾¼ê³µ #ì”ë§ìˆ˜
#ë¬´ë£Œì›¹íˆ° #ì›¹íˆ°ì¶”ì²œ #ì›¹íˆ°ë³´ëŠ”ê³³ #BLë³´ëŠ”ê³³

í”ŒëŸ¬íŒ… ì¥ì¸ì´ ì‘ì •í•˜ê³  ê¼¬ì‹œë©´ ì¼ì–´ë‚˜ëŠ” ì¼.. ì§€ì°½ìš± ì¹˜ëª…ì ì¸ FOX ëª¨ë¨¼íŠ¸.zip #ìˆ˜ìƒí•œíŒŒíŠ¸ë„ˆ #ëª¨ì•˜ìºì¹˜ #SBSCatch #kkuljaem #SBSìˆ˜ìƒí•œíŒŒíŠ¸ë„ˆ #ë“œë¼ë§ˆë‹¤ì‹œë³´ê¸° #ë“œë¼ë§ˆìš”ì•½ #ë“œë¼ë§ˆì¶”ì²œ #ë“œë¼ë§ˆí‚¤ìŠ¤ #ìˆ˜ìƒí•œíŒŒíŠ¸ë„ˆ1íšŒ #ìˆ˜ìƒí•œíŒŒíŠ¸ë„ˆëª…ì¥ë©´ #ìˆ˜ìƒí•œíŒŒíŠ¸ë„ˆëª°ì•„ë³´ê¸° #ìˆ˜ìƒí•œíŒŒíŠ¸ë„ˆìš”ì•½ #ìˆ˜ìƒí•œíŒŒíŠ¸ë„ˆì§€ì°½ìš± #ìˆ˜ìƒí•œíŒŒíŠ¸ë„ˆì§€ì°½ìš±ë‚¨ì§€í˜„ #ìŠ¤ë¸ŒìŠ¤ìºì¹˜ #ìŠ¤ë¸ŒìŠ¤ìºì¹˜ë“œë¼ë§ˆ #ì§€ì°½ìš±ë“œë¼ë§ˆ #ì§€ì°½ìš±ì—°ê¸°

ë©”ì´ì €â”ƒHello(êµ¬ ON) í† ì§€ë…¸ ì£¼ì†Œ scs33.com ì½”ë“œ HH22 , ì²«ê°€ì… 30%, ìŠ¤í¬ì¸  ë§¤ì¼ 10%, ì¹´ì§€ë…¸ ë§¤ì¼ 5% í¬ì¸íŠ¸ ì§€ê¸‰

ì£¼ì†Œ http://scs33.com ì½”ë“œ HH22

#Hello(í—¬ë¡œ) #í† í† ì‚¬ì´íŠ¸ #ë©”ì´ì €ë†€ì´í„° #ì•ˆì „ë†€ì´í„° #ë©”ì´ì €ì‚¬ì´íŠ¸ #ì¹´ì§€ë…¸ #ë°”ì¹´ë¼ #ìŠ¬ë¡¯ #ë¯¸ë‹ˆê²Œì„ #ë¸”ë™íˆ°


"""
# Tokenization (simple whitespace-based)
tokens = re.findall(r"\b\w+\b", text.lower())

# Calculate token frequencies
token_counts = Counter(tokens)

# Example: Filter common English stop words (you may need a list for your specific use case)
stop_words = set(["the", "and", "is", "in", "it", "to", "of", "for", "on", "with"])

# Filter out stop words
filtered_tokens = [token for token in tokens if token not in stop_words]

# Rank tokens by frequency
token_ranking = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)

# Choose the top N tokens as keywords (adjust N as needed)
top_keywords = [token for token, count in token_ranking[:10]]

# Print the extracted keywords
print(top_keywords)
